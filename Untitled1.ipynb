{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "547403ea6ad04679a81c38a8bc2a7f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b9a5e6218d448209dc370de5cc8e7b3",
              "IPY_MODEL_06db88bc08cf46fb89821959646c797d",
              "IPY_MODEL_16a419c0b890459685e2100d32f7333a"
            ],
            "layout": "IPY_MODEL_491691b66d0b4135af8c7ebfd91171c4"
          }
        },
        "5b9a5e6218d448209dc370de5cc8e7b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bffcfec53f64a3bb02c98fec95702df",
            "placeholder": "​",
            "style": "IPY_MODEL_61b770ce3ec247b493c3a4312212c195",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "06db88bc08cf46fb89821959646c797d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d07efcb780ad4b4ebedaad375b9ee8e5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97956adcbb484b3798b4c2a421e28eb7",
            "value": 2
          }
        },
        "16a419c0b890459685e2100d32f7333a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_520b179d793c43f6869ea5d63894098b",
            "placeholder": "​",
            "style": "IPY_MODEL_ca276fcd6614449789cd89150ce2c8b7",
            "value": " 2/2 [01:20&lt;00:00, 36.92s/it]"
          }
        },
        "491691b66d0b4135af8c7ebfd91171c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bffcfec53f64a3bb02c98fec95702df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61b770ce3ec247b493c3a4312212c195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d07efcb780ad4b4ebedaad375b9ee8e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97956adcbb484b3798b4c2a421e28eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "520b179d793c43f6869ea5d63894098b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca276fcd6614449789cd89150ce2c8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "513ce3922ed544faaf0f98aa3cdd368f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef1202b1639849a0ad8e28b5296f753d",
              "IPY_MODEL_79bb8dc106544befa594c22590f05030",
              "IPY_MODEL_362a5d2a81c94f6785309feac9f3ce5c"
            ],
            "layout": "IPY_MODEL_8d74e5c5d8e4478f86ec2dc6a0ddbe58"
          }
        },
        "ef1202b1639849a0ad8e28b5296f753d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d24a50da9fa04509a16899d30c0edc24",
            "placeholder": "​",
            "style": "IPY_MODEL_477d463a445f450b8f0426d24815cdb9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "79bb8dc106544befa594c22590f05030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbf75db9f6404d6f81b8fb944f2989f3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8751178baf884f069efb65f0c5184bdb",
            "value": 2
          }
        },
        "362a5d2a81c94f6785309feac9f3ce5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba62f9a1b76f49668f13b94d8a523d03",
            "placeholder": "​",
            "style": "IPY_MODEL_7e1011c1dc4540b1a97f95739fb9cafc",
            "value": " 2/2 [01:21&lt;00:00, 37.04s/it]"
          }
        },
        "8d74e5c5d8e4478f86ec2dc6a0ddbe58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24a50da9fa04509a16899d30c0edc24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477d463a445f450b8f0426d24815cdb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbf75db9f6404d6f81b8fb944f2989f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8751178baf884f069efb65f0c5184bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba62f9a1b76f49668f13b94d8a523d03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1011c1dc4540b1a97f95739fb9cafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U transformers accelerate peft bitsandbytes datasets\n"
      ],
      "metadata": {
        "id": "O3ZWm4ylH5gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip \"/content/Dataset.zip\""
      ],
      "metadata": {
        "id": "WPt82LgpHYrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(\"hf_token\")"
      ],
      "metadata": {
        "id": "q0uGHbWTIoqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    CLIPVisionModel, CLIPImageProcessor,\n",
        "    LlamaForCausalLM, LlamaTokenizer,\n",
        "    AutoProcessor, TrainingArguments,\n",
        "    Trainer, AutoTokenizer\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers.utils import logging\n",
        "import os\n",
        "logging.set_verbosity_error()  # Suppress too much logging\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "EfDmo_1nJAON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IpJWDCJPPaWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CONFIG ----\n",
        "image_encoder_name = \"openai/clip-vit-base-patch32\"\n",
        "llm_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "csv_path = \"/content/emotion_data.csv\"  # your dataset\n",
        "image_root = \"/content/Dataset/\"        # image folder\n",
        "max_length = 128\n",
        "batch_size = 4\n",
        "num_epochs = 3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "qPNCnMyxJGKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------- Load Vision Encoder ------- #\n",
        "vision_encoder = CLIPVisionModel.from_pretrained(image_encoder_name)\n",
        "vision_encoder = vision_encoder.to(device).eval()\n",
        "vision_encoder.float()  # Ensure float32 on GPU\n",
        "vision_encoder.requires_grad_(False)\n",
        "vision_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "\n",
        "# ---- TOKENIZER ----\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# ------- Load LLaMA Language Model ------- #\n",
        "language_model = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "language_model.gradient_checkpointing_enable()\n",
        "language_model = prepare_model_for_kbit_training(language_model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "language_model = get_peft_model(language_model, lora_config)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "547403ea6ad04679a81c38a8bc2a7f4b",
            "5b9a5e6218d448209dc370de5cc8e7b3",
            "06db88bc08cf46fb89821959646c797d",
            "16a419c0b890459685e2100d32f7333a",
            "491691b66d0b4135af8c7ebfd91171c4",
            "4bffcfec53f64a3bb02c98fec95702df",
            "61b770ce3ec247b493c3a4312212c195",
            "d07efcb780ad4b4ebedaad375b9ee8e5",
            "97956adcbb484b3798b4c2a421e28eb7",
            "520b179d793c43f6869ea5d63894098b",
            "ca276fcd6614449789cd89150ce2c8b7"
          ]
        },
        "id": "8ADyVJXjJMCr",
        "outputId": "e65edb3b-97eb-457b-98e2-ac59704a2ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "547403ea6ad04679a81c38a8bc2a7f4b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy forward to get dims\n",
        "with torch.no_grad():\n",
        "    dummy_out = vision_encoder(torch.randn(1, 3, 224, 224))\n",
        "vision_dim = dummy_out.last_hidden_state.shape[-1]\n",
        "llama_dim = language_model.model.embed_tokens.embedding_dim\n",
        "\n",
        "vision_proj = VisionProjector(vision_dim, llama_dim).to(language_model.device)\n",
        "vision_proj = get_peft_model(vision_proj, LoraConfig(\n",
        "    r=8, lora_alpha=16, lora_dropout=0.1,\n",
        "    target_modules=[\"proj\"],\n",
        "    bias=\"none\", task_type=\"FEATURE_EXTRACTION\"\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "S793w7Z1PAF8",
        "outputId": "8a3fc839-41e0-468f-a729-804d0c62694a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'LlamaForCausalLM' object has no attribute 'embed_tokens'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-ab16ccd96799>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdummy_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvision_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvision_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mllama_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvision_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisionProjector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvision_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllama_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1926\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1928\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1929\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'embed_tokens'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_processor, tokenizer, max_length=128):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_processor = image_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image = Image.open(\"/content/Dataset/\"+row['image_path']).convert(\"RGB\")\n",
        "        image_tensor = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        # Construct the full label string here\n",
        "        full_label_text = f'you look {row[\"emotion\"]}. {row[\"feedback\"]}'\n",
        "\n",
        "        # Pass the constructed string directly to the tokenizer\n",
        "        label = self.tokenizer(\n",
        "            full_label_text, padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt'\n",
        "        )\n",
        "        data_return = {\n",
        "\n",
        "                \"pixel_values\": image_tensor,\n",
        "                \"input_ids\": label[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": label[\"attention_mask\"].squeeze(0)\n",
        "            }\n",
        "        # print(data_return)\n",
        "        return data_return\n",
        "\n",
        "def custom_data_collator(features):\n",
        "    # print(\"Features:\", features)\n",
        "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
        "    input_ids = torch.stack([f[\"input_ids\"] for f in features])\n",
        "    attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask\n",
        "    }\n",
        "\n",
        "# # --------- Dataset Loading ----------\n",
        "# tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "dataset = ImageTextDataset(\n",
        "    csv_path=\"/content/Dataset/image_sentiment_dataset.csv\",  # CSV with columns: image_path,text\n",
        "    image_processor=vision_processor,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=max_length\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_data_collator)\n"
      ],
      "metadata": {
        "id": "JLat7shnJPMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vision_dim = vision_encoder.config.hidden_size\n",
        "text_dim = language_model.config.hidden_size\n",
        "projector = nn.Linear(vision_encoder.config.hidden_size, language_model.config.hidden_size)\n",
        "projector = projector.to(device).float()\n"
      ],
      "metadata": {
        "id": "ie10JnFsa4ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(list(language_model.parameters()) + list(projector.parameters()), lr=2e-5)\n"
      ],
      "metadata": {
        "id": "un6EPdACa_AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- TRAIN LOOP ----\n",
        "language_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vision_outputs = vision_encoder(pixel_values=pixel_values)\n",
        "            image_embeds = vision_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        prefix_embeds = projector(image_embeds).unsqueeze(1)\n",
        "        text_embeds = language_model.model.model.embed_tokens(input_ids)  # ✅\n",
        "\n",
        "        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)\n",
        "\n",
        "        # Adjust attention mask accordingly\n",
        "        batch_size, seq_len = input_ids.size()\n",
        "        prefix_attention = torch.ones((batch_size, 1), device=device)\n",
        "        combined_attention_mask = torch.cat([prefix_attention, attention_mask], dim=1)\n",
        "\n",
        "        print(\"inputs_embeds shape:\", inputs_embeds.shape)  # (batch_size, seq_len, embed_dim)\n",
        "        print(\"labels shape:\", input_ids.shape)                # (batch_size, seq_len)\n",
        "        print(\"attention_mask shape:\", combined_attention_mask.shape)\n",
        "        inputs_embeds = inputs_embeds[:, :128, :]\n",
        "        attention_mask = attention_mask[:, :128]\n",
        "        outputs = language_model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=combined_attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"✅ Epoch {epoch+1} complete | Avg Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzVwRD0sLYRk",
        "outputId": "4a06262e-9320-4c1e-a915-72bee3b4ed29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/3:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  20%|██        | 1/5 [00:06<00:25,  6.27s/it, loss=25.9]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  40%|████      | 2/5 [00:12<00:18,  6.19s/it, loss=22.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  60%|██████    | 3/5 [00:18<00:12,  6.21s/it, loss=24.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3:  80%|████████  | 4/5 [00:25<00:06,  6.28s/it, loss=23.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 5/5 [00:31<00:00,  6.31s/it, loss=20.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 1 complete | Avg Loss: 23.2980\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 2/3:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  20%|██        | 1/5 [00:06<00:26,  6.66s/it, loss=19.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  40%|████      | 2/5 [00:13<00:20,  6.74s/it, loss=19.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  60%|██████    | 3/5 [00:20<00:13,  6.77s/it, loss=20]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3:  80%|████████  | 4/5 [00:26<00:06,  6.72s/it, loss=18.5]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 5/5 [00:33<00:00,  6.69s/it, loss=17.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 2 complete | Avg Loss: 19.0475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3/3:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  20%|██        | 1/5 [00:06<00:25,  6.37s/it, loss=17.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  40%|████      | 2/5 [00:12<00:19,  6.34s/it, loss=13.1]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  60%|██████    | 3/5 [00:18<00:12,  6.31s/it, loss=10.9]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3:  80%|████████  | 4/5 [00:25<00:06,  6.28s/it, loss=10.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_embeds shape: torch.Size([4, 129, 4096])\n",
            "labels shape: torch.Size([4, 128])\n",
            "attention_mask shape: torch.Size([4, 129])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 5/5 [00:31<00:00,  6.29s/it, loss=9.66]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 3 complete | Avg Loss: 12.2557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSh-nDc5LcLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import CLIPProcessor, CLIPModel, LlamaTokenizer, LlamaForCausalLM\n",
        "# from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load models and tokenizer (adjust paths/names accordingly)\n",
        "# vision_encoder = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").vision_model.eval().to(device)\n",
        "# tokenizer = LlamaTokenizer.from_pretrained(\"your-llama-model\")\n",
        "# language_model = language_model.to(device)\n",
        "# If using LoRA with PEFT:\n",
        "# from peft import PeftModel\n",
        "# language_model = PeftModel.from_pretrained(language_model, \"path-to-lora-checkpoint\").to(device)\n",
        "\n",
        "# Load processor for images (CLIPProcessor or your image_processor)\n",
        "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assume these are your models\n",
        "vision_embed_dim = 4096\n",
        "llama_embed_dim = 768\n",
        "\n",
        "# Define a projection layer\n",
        "image_projection = nn.Linear(vision_embed_dim, llama_embed_dim).to(device)\n",
        "\n",
        "def generate_text_from_image(image_path, max_new_tokens=50):\n",
        "    # Load and process the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = vision_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    pixel_values = inputs[\"pixel_values\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vision_outputs = vision_encoder(pixel_values=pixel_values)\n",
        "        image_embeds = vision_outputs.last_hidden_state.mean(dim=1)  # shape: [1, 4096]\n",
        "\n",
        "    # Project image embeddings to llama embed dim\n",
        "    projected_image_embeds = projector(image_embeds)  # shape: [1, 768]\n",
        "\n",
        "    # Prepare prompt\n",
        "    prompt = \"Describe the image:\"\n",
        "    inputs_tokenized = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    input_ids = inputs_tokenized.input_ids\n",
        "    attention_mask = inputs_tokenized.attention_mask\n",
        "\n",
        "    inputs_embeds = language_model.get_input_embeddings()(input_ids)  # shape: [1, seq_len, 768]\n",
        "\n",
        "    # Add a sequence dimension to projected image embeds: [batch_size, 1, embed_dim]\n",
        "    projected_image_embeds = projected_image_embeds.unsqueeze(1)  # shape: [1,1,768]\n",
        "\n",
        "    # Concatenate along sequence length dim\n",
        "    inputs_embeds = torch.cat([projected_image_embeds, inputs_embeds], dim=1)\n",
        "\n",
        "    # Adjust attention mask\n",
        "    extended_attention_mask = torch.cat(\n",
        "        [torch.ones((attention_mask.size(0), 1), device=device), attention_mask], dim=1\n",
        "    )\n",
        "\n",
        "    # Generate text\n",
        "    generated_ids = language_model.generate(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=extended_attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/Dataset/images/116.jpg\"\n",
        "result = generate_text_from_image(image_path)\n",
        "print(\"Generated text:\", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKMui9qHgK6Z",
        "outputId": "ccda9df2-b5f6-42dc-abc4-c58ca3df3da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: \n",
            "persuaded the persuaded the persun. to make a man. to make a man. to make a man of men. to make a man of men. to make a man of men. to make a man of men. to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feedback(image_path, max_new_tokens=30):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_value = vision_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vision_feat = vision_encoder(pixel_value.unsqueeze(0)).last_hidden_state.mean(dim=1)\n",
        "        prefix_embed = projector(vision_feat).unsqueeze(1)\n",
        "\n",
        "        input_ids = tokenizer(\"<s>\", return_tensors=\"pt\").input_ids.to(device)\n",
        "        token_embed = language_model.model.model.embed_tokens(input_ids)\n",
        "        input_embed = torch.cat([prefix_embed, token_embed], dim=1)\n",
        "\n",
        "        output_ids = language_model.generate(\n",
        "            inputs_embeds=input_embed,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=1.0,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# --- Example Usage ---\n",
        "if __name__ == \"__main__\":\n",
        "    test_image = \"/content/Dataset/images/116.jpg\"  # Replace with your image path\n",
        "    result = generate_feedback(test_image)\n",
        "    print(\"\\n🧠 Generated Feedback:\")\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vfn9N0phZVV",
        "outputId": "3829a927-3cbe-491c-8ad0-0b96bceed8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧠 Generated Feedback:\n",
            "Gestation 400 is the Latin alphabet, derived from Old Latin a 300 400 30 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#🧠 Generated Feedback:\n",
        "#Gestation 400 is the Latin alphabet, derived from Old Latin a 300 400 30 30\n"
      ],
      "metadata": {
        "id": "280Q3lRQk2Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fQIFBeWk2FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xMvuQVu-k2Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEGbnINlk2AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- CONFIG ----\n",
        "image_encoder_name = \"openai/clip-vit-base-patch32\"\n",
        "llm_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "csv_path = \"/content/Dataset/image_sentiment_dataset.csv\"\n",
        "image_root = \"/content/Dataset/\"\n",
        "max_length = 128\n",
        "batch_size = 4\n",
        "num_epochs = 3\n",
        "lr = 2e-5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---- Load Vision Encoder ----\n",
        "vision_encoder = CLIPVisionModel.from_pretrained(image_encoder_name).to(device)\n",
        "vision_encoder.eval().requires_grad_(False)\n",
        "vision_processor = CLIPImageProcessor.from_pretrained(image_encoder_name)\n",
        "\n",
        "# ---- Load Tokenizer ----\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ---- Load LLaMA Language Model with LoRA + 4-bit ----\n",
        "language_model = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "language_model.gradient_checkpointing_enable()\n",
        "language_model = prepare_model_for_kbit_training(language_model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "language_model = get_peft_model(language_model, lora_config)\n",
        "\n",
        "# ---- Dataset ----\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_processor, tokenizer, max_length=128):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_processor = image_processor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = os.path.join(image_root, row['image_path'])\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image_tensor = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        full_label = f'you look {row[\"emotion\"]}. {row[\"feedback\"]}'\n",
        "        label_encoding = self.tokenizer(full_label, padding='max_length', truncation=True,\n",
        "                                        max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": image_tensor,\n",
        "            \"input_ids\": label_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": label_encoding[\"attention_mask\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "def custom_collate(features):\n",
        "    return {\n",
        "        \"pixel_values\": torch.stack([f[\"pixel_values\"] for f in features]),\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features])\n",
        "    }\n",
        "\n",
        "# ---- Load Data ----\n",
        "dataset = ImageTextDataset(csv_path, vision_processor, tokenizer, max_length)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
        "\n",
        "# ---- Projector ----\n",
        "projector = nn.Sequential(\n",
        "    nn.Linear(vision_encoder.config.hidden_size, language_model.config.hidden_size),\n",
        "    nn.Tanh()\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(list(language_model.parameters()) + list(projector.parameters()), lr=lr)\n",
        "\n",
        "# ---- Training Loop ----\n",
        "language_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vision_outputs = vision_encoder(pixel_values=pixel_values)\n",
        "            image_embeds = vision_outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        prefix_embeds = projector(image_embeds).unsqueeze(1)\n",
        "        text_embeds = language_model.model.model.embed_tokens(input_ids)\n",
        "        inputs_embeds = torch.cat([prefix_embeds, text_embeds], dim=1)\n",
        "\n",
        "        # Adjust attention and labels\n",
        "        prefix_attention = torch.ones((input_ids.size(0), 1), device=device)\n",
        "        combined_attention_mask = torch.cat([prefix_attention, attention_mask], dim=1)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels = torch.cat([torch.full((labels.size(0), 1), -100, dtype=torch.long, device=device), labels[:, :-1]], dim=1)\n",
        "\n",
        "        # Truncate to max length\n",
        "        inputs_embeds = inputs_embeds[:, :max_length, :]\n",
        "        combined_attention_mask = combined_attention_mask[:, :max_length]\n",
        "        labels = labels[:, :max_length]\n",
        "\n",
        "        outputs = language_model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=combined_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print(f\"✅ Epoch {epoch+1} complete | Avg Loss: {running_loss / len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "513ce3922ed544faaf0f98aa3cdd368f",
            "ef1202b1639849a0ad8e28b5296f753d",
            "79bb8dc106544befa594c22590f05030",
            "362a5d2a81c94f6785309feac9f3ce5c",
            "8d74e5c5d8e4478f86ec2dc6a0ddbe58",
            "d24a50da9fa04509a16899d30c0edc24",
            "477d463a445f450b8f0426d24815cdb9",
            "dbf75db9f6404d6f81b8fb944f2989f3",
            "8751178baf884f069efb65f0c5184bdb",
            "ba62f9a1b76f49668f13b94d8a523d03",
            "7e1011c1dc4540b1a97f95739fb9cafc"
          ]
        },
        "id": "OPj7_538k19y",
        "outputId": "3f9b11b7-e0bf-4f66-f72a-5a449d6f175c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "513ce3922ed544faaf0f98aa3cdd368f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/3:   0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 1/3: 100%|██████████| 5/5 [00:34<00:00,  6.99s/it, loss=6.86]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 1 complete | Avg Loss: 12.3480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 5/5 [00:31<00:00,  6.30s/it, loss=7.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 2 complete | Avg Loss: 7.5954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 5/5 [00:32<00:00,  6.47s/it, loss=5.88]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 3 complete | Avg Loss: 6.2044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the LoRA-adapted LLaMA model\n",
        "language_model.save_pretrained(\"/content/saved_model/vlm_lora_llama2\")\n",
        "\n",
        "# Save the projector layer separately\n",
        "torch.save(projector.state_dict(), \"/content/vlm_projector.pt\")\n"
      ],
      "metadata": {
        "id": "BI9rtz7ek4Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Load LLaMA base in 4-bit and apply LoRA weights\n",
        "base_model = LlamaForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, \"/content/vlm_lora_llama2\").eval()\n",
        "\n",
        "# Load projector\n",
        "projector = nn.Sequential(\n",
        "    nn.Linear(vision_encoder.config.hidden_size, model.config.hidden_size),\n",
        "    nn.Tanh()\n",
        ").to(device)\n",
        "projector.load_state_dict(torch.load(\"/content/vlm_projector.pt\", map_location=device))\n"
      ],
      "metadata": {
        "id": "ic0vk2Q1l-qI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}